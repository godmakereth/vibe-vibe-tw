---
title: "12.6.2 別把對方網站爬崩了——速率控制：請求頻率與併發限制"
typora-root-url: ../../public
---

# 12.6.2 別把對方網站爬崩了——速率控制：請求頻率與併發限制

### 一句話破題

速率控制的核心是"不要貪婪"——你的爬蟲只是目標網站的無數訪問者之一，不應該佔用過多資源。

### 速率限制器

```typescript
class RateLimiter {
  private lastRequest = 0;
  private minInterval: number;

  constructor(requestsPerSecond: number) {
    this.minInterval = 1000 / requestsPerSecond;
  }

  async wait() {
    const now = Date.now();
    const elapsed = now - this.lastRequest;
    const waitTime = Math.max(0, this.minInterval - elapsed);
    
    if (waitTime > 0) {
      await new Promise(r => setTimeout(r, waitTime));
    }
    
    this.lastRequest = Date.now();
  }
}

// 使用示例：每秒最多 2 個請求
const limiter = new RateLimiter(2);

async function fetchWithRateLimit(url: string) {
  await limiter.wait();
  return fetch(url);
}
```

### 併發控制

```typescript
class ConcurrencyLimiter {
  private running = 0;
  private queue: (() => void)[] = [];

  constructor(private maxConcurrent: number) {}

  async acquire() {
    if (this.running >= this.maxConcurrent) {
      await new Promise<void>(resolve => this.queue.push(resolve));
    }
    this.running++;
  }

  release() {
    this.running--;
    const next = this.queue.shift();
    if (next) next();
  }

  async run<T>(fn: () => Promise<T>): Promise<T> {
    await this.acquire();
    try {
      return await fn();
    } finally {
      this.release();
    }
  }
}

// 使用示例：最多 5 個併發請求
const concurrency = new ConcurrencyLimiter(5);

async function crawlUrls(urls: string[]) {
  return Promise.all(
    urls.map(url => concurrency.run(() => fetch(url)))
  );
}
```

### 綜合爬蟲調度器

```typescript
interface CrawlerConfig {
  maxConcurrent: number;
  requestsPerSecond: number;
  retryAttempts: number;
  timeout: number;
}

class Crawler {
  private rateLimiter: RateLimiter;
  private concurrency: ConcurrencyLimiter;
  
  constructor(private config: CrawlerConfig) {
    this.rateLimiter = new RateLimiter(config.requestsPerSecond);
    this.concurrency = new ConcurrencyLimiter(config.maxConcurrent);
  }

  async fetch(url: string): Promise<Response> {
    return this.concurrency.run(async () => {
      await this.rateLimiter.wait();
      
      const controller = new AbortController();
      const timeout = setTimeout(() => controller.abort(), this.config.timeout);
      
      try {
        const response = await fetch(url, { signal: controller.signal });
        return response;
      } finally {
        clearTimeout(timeout);
      }
    });
  }

  async crawl(urls: string[], onResult: (url: string, data: unknown) => void) {
    const results = await Promise.allSettled(
      urls.map(async url => {
        const response = await this.fetch(url);
        const data = await response.text();
        onResult(url, data);
        return data;
      })
    );
    
    return results;
  }
}

// 使用示例
const crawler = new Crawler({
  maxConcurrent: 5,
  requestsPerSecond: 2,
  retryAttempts: 3,
  timeout: 10000,
});
```

### 指數退避

當遇到 429（Too Many Requests）時：

```typescript
async function fetchWithBackoff(url: string, maxRetries = 5) {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(url);
    
    if (response.status === 429) {
      const retryAfter = response.headers.get('Retry-After');
      const delay = retryAfter 
        ? parseInt(retryAfter) * 1000 
        : Math.pow(2, attempt) * 1000;
      
      console.log(`收到 429，等待 ${delay}ms 後重試`);
      await new Promise(r => setTimeout(r, delay));
      continue;
    }
    
    return response;
  }
  
  throw new Error('重試次數已用盡');
}
```

### AI 協作指南

- **核心意圖**：讓 AI 幫你實現合理的爬蟲速率控制。
- **需求定義公式**：`"請幫我實現一個爬蟲調度器，支持每秒最多 2 個請求、最多 5 個併發，並能處理 429 響應的指數退避。"`
- **關鍵術語**：`速率限制 (rate limiting)`、`併發控制`、`指數退避 (exponential backoff)`、`429 Too Many Requests`

### 避坑指南

- **遵守 Crawl-delay**：如果 robots.txt 指定了延遲，務必遵守。
- **監控響應碼**：429 和 503 通常表示你太快了。
- **隨機化間隔**：添加隨機抖動，避免請求模式過於規律。
