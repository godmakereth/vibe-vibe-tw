---
title: "12.6.1 爬蟲的規矩——爬蟲倫理：robots.txt 與網站條款"
typora-root-url: ../../public
---

# 12.6.1 爬蟲的規矩——爬蟲倫理：robots.txt 與網站條款

### 一句話破題

robots.txt 是網站對爬蟲的"門禁規則"——雖然不是強制執行的法律，但遵守它是爬蟲開發者的基本禮儀。

### robots.txt 解讀

```
# https://example.com/robots.txt
User-agent: *        # 適用於所有爬蟲
Disallow: /admin/    # 禁止爬取 /admin/ 路徑
Disallow: /private/  # 禁止爬取 /private/ 路徑
Allow: /public/      # 允許爬取 /public/ 路徑
Crawl-delay: 10      # 建議每次請求間隔 10 秒

User-agent: Googlebot  # 專門針對 Google 爬蟲的規則
Allow: /               # 允許爬取所有內容

Sitemap: https://example.com/sitemap.xml  # 站點地圖位置
```

### 解析 robots.txt

```typescript
interface RobotRule {
  userAgent: string;
  allow: string[];
  disallow: string[];
  crawlDelay?: number;
}

async function parseRobotsTxt(domain: string): Promise<RobotRule[]> {
  const response = await fetch(`${domain}/robots.txt`);
  const text = await response.text();
  
  const rules: RobotRule[] = [];
  let currentRule: RobotRule | null = null;
  
  for (const line of text.split('\n')) {
    const trimmed = line.trim();
    if (trimmed.startsWith('#') || !trimmed) continue;
    
    const [key, ...valueParts] = trimmed.split(':');
    const value = valueParts.join(':').trim();
    
    switch (key.toLowerCase()) {
      case 'user-agent':
        if (currentRule) rules.push(currentRule);
        currentRule = { userAgent: value, allow: [], disallow: [] };
        break;
      case 'allow':
        currentRule?.allow.push(value);
        break;
      case 'disallow':
        currentRule?.disallow.push(value);
        break;
      case 'crawl-delay':
        if (currentRule) currentRule.crawlDelay = parseInt(value);
        break;
    }
  }
  
  if (currentRule) rules.push(currentRule);
  return rules;
}

function isPathAllowed(rules: RobotRule[], path: string, userAgent = '*'): boolean {
  const rule = rules.find(r => r.userAgent === userAgent) || rules.find(r => r.userAgent === '*');
  if (!rule) return true;
  
  for (const disallowed of rule.disallow) {
    if (path.startsWith(disallowed)) return false;
  }
  return true;
}
```

### 服務條款注意事項

除了 robots.txt，還需要關注網站的服務條款（ToS）：

| 條款類型 | 示例 | 風險等級 |
|----------|------|----------|
| 禁止自動化訪問 | "不得使用自動化工具訪問本網站" | 高 |
| 數據使用限制 | "不得將數據用於商業用途" | 高 |
| 頻率限制 | "每分鐘不得超過 100 次請求" | 中 |
| 版權聲明 | "所有內容版權歸本網站所有" | 高 |

### 合法爬取的原則

1. **遵守 robots.txt**：這是最基本的禮儀
2. **閱讀服務條款**：瞭解網站的具體限制
3. **控制請求頻率**：不要給網站帶來負擔
4. **標識你的爬蟲**：設置合理的 User-Agent
5. **尊重版權**：不要複製受版權保護的內容

### AI 協作指南

- **核心意圖**：讓 AI 幫你理解和遵守爬蟲規則。
- **需求定義公式**：`"請幫我解析這個 robots.txt 文件，並判斷我是否可以爬取 /products 路徑。"`
- **關鍵術語**：`robots.txt`、`User-Agent`、`Disallow`、`Crawl-delay`

### 避坑指南

- **robots.txt 是君子協定**：技術上可以忽略，但會帶來法律和道德風險。
- **不同 User-Agent 規則不同**：確認你的爬蟲身份適用哪條規則。
- **404 不代表允許**：robots.txt 不存在不代表可以隨意爬取。
